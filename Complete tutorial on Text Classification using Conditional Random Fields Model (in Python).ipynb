{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Tag\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pycrfsuite\n",
    "import os, os.path, sys\n",
    "import glob\n",
    "from xml.etree import ElementTree\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function appends all annotated files\n",
    "def append_annotations(files):\n",
    "    xml_files = glob.glob(files +\"/*.xml\")\n",
    "    xml_element_tree = None\n",
    "    new_data = \"\"\n",
    "    for xml_file in xml_files:\n",
    "        data = ElementTree.parse(xml_file).getroot()\n",
    "        #print ElementTree.tostring(data)        \n",
    "        temp = ElementTree.tostring(data)\n",
    "        new_data += (temp)\n",
    "    return(new_data)\n",
    "\n",
    "#this function removes special characters and punctuations\n",
    "def remov_punct(withpunct):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    without_punct = \"\"\n",
    "    char = 'nan'\n",
    "    for char in withpunct:\n",
    "        if char not in punctuations:\n",
    "            without_punct = without_punct + char\n",
    "    return(without_punct)\n",
    "\n",
    "# functions for extracting features in documents\n",
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def get_labels(doc):\n",
    "    return [label for (token, postag, label) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = \"D:/Annotated/\"\n",
    "\n",
    "allxmlfiles = append_annotations(files_path)\n",
    "soup = bs(allxmlfiles, \"html5lib\")\n",
    "\n",
    "#identify the tagged element\n",
    "docs = []\n",
    "sents = []\n",
    "\n",
    "for d in soup.find_all(\"document\"):\n",
    "   for wrd in d.contents:    \n",
    "    tags = []\n",
    "    NoneType = type(None)   \n",
    "    if isinstance(wrd.name, NoneType) == True:\n",
    "        withoutpunct = remov_punct(wrd)\n",
    "        temp = word_tokenize(withoutpunct)\n",
    "        for token in temp:\n",
    "            tags.append((token,'NA'))            \n",
    "    else:\n",
    "        withoutpunct = remov_punct(wrd)\n",
    "        temp = word_tokenize(withoutpunct)\n",
    "        for token in temp:\n",
    "            tags.append((token,wrd.name))    \n",
    "    sents = sents + tags \n",
    "   docs.append(sents) #appends all the individual documents into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    tokens = [t for t, label in doc]    \n",
    "    tagged = nltk.pos_tag(tokens)    \n",
    "    data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])\n",
    "\n",
    "def word2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "\n",
    "# Common features for all words. You may add more features here based on your custom use case\n",
    "features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag\n",
    "    ]\n",
    "\n",
    "# Features for words that are not at the beginning of a document\n",
    "if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "# Features for words that are not at the end of a document\n",
    "if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    " return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [extract_features(doc) for doc in data]\n",
    "y = [get_labels(doc) for doc in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('crf.model')\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "\n",
    "    print(\"%s (%s)\" % (y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of labels to indices\n",
    "labels = {\"claim_number\": 1, \"claimant\": 1,\"NA\": 0}\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_test for tag in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=[\"claim_number\", \"claimant\",\"NA\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict new data\n",
    "with codecs.open(\"D:/ SampleEmail6.xml\", \"r\", \"utf-8\") as infile:\n",
    "    soup_test = bs(infile, \"html5lib\")\n",
    "\n",
    "docs = []\n",
    "sents = []\n",
    "\n",
    "for d in soup_test.find_all(\"document\"):\n",
    "   for wrd in d.contents:    \n",
    "    tags = []\n",
    "    NoneType = type(None)   \n",
    "\n",
    "    if isinstance(wrd.name, NoneType) == True:\n",
    "        withoutpunct = remov_punct(wrd)\n",
    "        temp = word_tokenize(withoutpunct)\n",
    "        for token in temp:\n",
    "            tags.append((token,'NA'))            \n",
    "    else:\n",
    "        withoutpunct = remov_punct(wrd)\n",
    "        temp = word_tokenize(withoutpunct)\n",
    "        for token in temp:\n",
    "            tags.append((token,wrd.name))\n",
    "    #docs.append(tags)\n",
    "\n",
    "sents = sents + tags # puts all the sentences of a document in one element of the list\n",
    "docs.append(sents) #appends all the individual documents into one list       \n",
    "\n",
    "data_test = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    tokens = [t for t, label in doc]    \n",
    "    tagged = nltk.pos_tag(tokens)    \n",
    "    data_test.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])\n",
    "\n",
    "data_test_feats = [extract_features(doc) for doc in data_test]\n",
    "tagger.open('crf.model')\n",
    "newdata_pred = [tagger.tag(xseq) for xseq in data_test_feats]\n",
    "\n",
    "# Let's check predicted data\n",
    "i = 0\n",
    "for x, y in zip(newdata_pred[i], [x[1].split(\"=\")[1] for x in data_test_feats[i]]):\n",
    "    print(\"%s (%s)\" % (y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
